{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Standard  \\\n",
      "0                 Standard-Quality Management System   \n",
      "1  Quality Management System Fundamental Standard...   \n",
      "2                 Standard-Quality Management System   \n",
      "3  Quality Management System Standard of Practice...   \n",
      "4                 Standard-Quality Management System   \n",
      "\n",
      "                                            Guidance  \n",
      "0                                           Guidance  \n",
      "1                                                NaN  \n",
      "2                                           Guidance  \n",
      "3  The Quality Management System (QMS) must inclu...  \n",
      "4                                           Guidance  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"regulationsfiltered.csv\")\n",
    "\n",
    "# Inspect the data\n",
    "print(df.head())\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# List to store LangChain Document objects\n",
    "documents = []\n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for _, row in df.iterrows():\n",
    "    reg_chunks = text_splitter.split_text(row[\"Standard\"]) if pd.notna(row[\"Standard\"]) else []\n",
    "    info_chunks = text_splitter.split_text(row[\"Guidance\"]) if pd.notna(row[\"Guidance\"]) else []\n",
    "\n",
    "    # Create LangChain Document objects with metadata\n",
    "    for chunk in reg_chunks:\n",
    "        documents.append(Document(page_content=chunk, metadata={\"source\": \"Standard\"}))\n",
    "    \n",
    "    for chunk in info_chunks:\n",
    "        documents.append(Document(page_content=chunk, metadata={\"source\": \"Guidance\"}))\n",
    "\n",
    "# Now, `documents` contains all processed chunks as LangChain `Document` objects\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
